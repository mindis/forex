{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "actor-critic-duel-agent.ipynb のコピー",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komo135/forex/blob/master/actor_critic_duel_agent_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiy5cu8MhYE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlJqncb9bDKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google ドライブをマウントするには、このセルを実行してください。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nzfWOyfsYwko",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "\n",
        "class Actor:\n",
        "    def __init__(self, name, input_size, output_size):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, input_size)\n",
        "            cnn1 = tf.layers.conv1d(self.X, 16, 2, padding=\"same\", activation=tf.nn.relu)\n",
        "            cnn2 = tf.layers.conv1d(self.X, 32, 2, padding=\"same\", activation=tf.nn.relu)\n",
        "            cnn3 = tf.layers.conv1d(self.X, 48, 2, padding=\"same\", activation=tf.nn.relu)\n",
        "            cnn4 = tf.layers.conv1d(self.X, 62, 2, padding=\"same\", activation=tf.nn.relu)\n",
        "            cnn5 = tf.layers.conv1d(self.X, 80, 2, padding=\"same\", activation=tf.nn.relu)\n",
        "            feed_actor = tf.keras.layers.concatenate([cnn1, cnn2, cnn3, cnn4, cnn5])\n",
        "            feed_actor = tf.layers.flatten(feed_actor)\n",
        "            tensor_action, tensor_validation = tf.split(feed_actor, 2, 1)\n",
        "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
        "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "            self.logits = feed_validation + tf.subtract(feed_action,\n",
        "                                                        tf.reduce_mean(feed_action, axis=1, keep_dims=True))\n",
        "\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, name, input_size, output_size, learning_rate):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, input_size)\n",
        "            self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
        "            self.REWARD = tf.placeholder(tf.float32, (None, 1))\n",
        "            cnn1 = tf.layers.conv1d(self.X, 16, 2, padding=\"same\", activation=tf.nn.relu)\n",
        "            cnn2 = tf.layers.conv1d(self.X, 32, 2, padding=\"same\", activation=tf.nn.relu)\n",
        "            cnn3 = tf.layers.conv1d(self.X, 48, 2, padding=\"same\", activation=tf.nn.relu)\n",
        "            cnn4 = tf.layers.conv1d(self.X, 62, 2, padding=\"same\", activation=tf.nn.relu)\n",
        "            cnn5 = tf.layers.conv1d(self.X, 80, 2, padding=\"same\", activation=tf.nn.relu)\n",
        "            feed_critic = tf.keras.layers.concatenate([cnn1, cnn2, cnn3, cnn4, cnn5])\n",
        "            feed_critic = tf.layers.flatten(feed_critic)\n",
        "            tensor_action, tensor_validation = tf.split(feed_critic, 2, 1)\n",
        "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
        "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "            feed_critic = feed_validation + tf.subtract(feed_action,\n",
        "                                                        tf.reduce_mean(feed_action, axis=1, keep_dims=True))\n",
        "            feed_critic = tf.nn.relu(feed_critic) + self.Y\n",
        "            feed_critic = tf.layers.dense(feed_critic, 128 // 2, activation=tf.nn.relu)\n",
        "            self.logits = tf.layers.dense(feed_critic, 1)\n",
        "            self.cost = tf.reduce_mean(tf.square(self.REWARD - self.logits))\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    LEARNING_RATE = 0.001\n",
        "    BATCH_SIZE = 32\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    MEMORY_SIZE = 5000\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "\n",
        "    def __init__(self, path, window_size, skip):\n",
        "        self.path = path\n",
        "        self.window_size = window_size\n",
        "        self._preproc()\n",
        "        self.state_size = (None, 1, self.df.shape[-1])\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.actor = Actor('actor-original', self.state_size, self.OUTPUT_SIZE)\n",
        "        self.actor_target = Actor('actor-target', self.state_size, self.OUTPUT_SIZE)\n",
        "        self.critic = Critic('critic-original', self.state_size, self.OUTPUT_SIZE, self.LEARNING_RATE)\n",
        "        self.critic_target = Critic('critic-target', self.state_size, self.OUTPUT_SIZE, self.LEARNING_RATE)\n",
        "        self.grad_critic = tf.gradients(self.critic.logits, self.critic.Y)\n",
        "        self.actor_critic_grad = tf.placeholder(tf.float32, [None, self.OUTPUT_SIZE])\n",
        "        weights_actor = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
        "        self.grad_actor = tf.gradients(self.actor.logits, weights_actor, -self.actor_critic_grad)\n",
        "        grads = zip(self.grad_actor, weights_actor)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE).apply_gradients(grads)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def _preproc(self):\n",
        "        df = pd.read_csv(self.path)\n",
        "        X = df[[\"Close\",\"ema4\",\"ema8\",\"ema16\"]]\n",
        "        X = preprocessing.MinMaxScaler().fit_transform(X)\n",
        "         \n",
        "        model = tf.keras.models.load_model(\"drive/My Drive/ema8.hdf5\")\n",
        "        gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(X, X, self.window_size, batch_size=1000)\n",
        "        pred = model.predict_generator(gen).reshape((-1,1,4))\n",
        "        \n",
        "        p = []\n",
        "        for i in range(1,len(pred)):\n",
        "          p.append(pred[i] - pred[i-1])\n",
        "        p = np.asanyarray(p)\n",
        "        y = np.asanyarray(df[[\"Open\"]])\n",
        "        y = y[-len(p)::]\n",
        "        \n",
        "        self.df = p\n",
        "        self.trend = y\n",
        "\n",
        "    def _assign(self, from_name, to_name):\n",
        "        from_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_name)\n",
        "        to_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_name)\n",
        "        for i in range(len(from_w)):\n",
        "            assign_op = to_w[i].assign(from_w[i])\n",
        "            self.sess.run(assign_op)\n",
        "\n",
        "    def _memorize(self, state, action, reward, new_state, dead):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, dead))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "\n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            prediction = self.sess.run(self.actor.logits, feed_dict={self.actor.X: [state]})[0]\n",
        "            action = np.argmax(prediction)\n",
        "        return action\n",
        "\n",
        "    def _construct_memories_and_train(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        Q = self.sess.run(self.actor.logits, feed_dict={self.actor.X: states})\n",
        "        Q_target = self.sess.run(self.actor_target.logits, feed_dict={self.actor_target.X: states})\n",
        "        grads = self.sess.run(self.grad_critic, feed_dict={self.critic.X: states, self.critic.Y: Q})[0]\n",
        "        self.sess.run(self.optimizer, feed_dict={self.actor.X: states, self.actor_critic_grad: grads})\n",
        "\n",
        "        rewards = np.array([a[2] for a in replay]).reshape((-1, 1))\n",
        "        rewards_target = self.sess.run(self.critic_target.logits,\n",
        "                                       feed_dict={self.critic_target.X: new_states, self.critic_target.Y: Q_target})\n",
        "        for i in range(len(replay)):\n",
        "            if not replay[0][-1]:\n",
        "                rewards[i] += self.GAMMA * rewards_target[i]\n",
        "        cost, _ = self.sess.run([self.critic.cost, self.critic.optimizer],\n",
        "                                feed_dict={self.critic.X: states, self.critic.Y: Q, self.critic.REWARD: rewards})\n",
        "        return cost\n",
        "\n",
        "    def get_state(self, t):\n",
        "        return self.df[t]\n",
        "\n",
        "    def buy(self, spread, pip_cost, sl):\n",
        "        position = 3\n",
        "        pip = 0\n",
        "        spread = spread / pip_cost\n",
        "        loscut = False\n",
        "\n",
        "        for t in range(len(self.trend) - 28000, len(self.trend), self.skip):\n",
        "            state = self.get_state(t)\n",
        "            action = self._select_action(state)\n",
        "            '''\n",
        "                0 == Hold\n",
        "                1 = buy\n",
        "                2 = sell\n",
        "                3 = start\n",
        "                '''\n",
        "            if action == 1:\n",
        "                if position == 3:\n",
        "                    states = self.trend[t] + spread\n",
        "                    position = 1\n",
        "                elif position == 2:\n",
        "                    pip += ((states - self.trend[t]) * pip_cost)\n",
        "                    states = self.trend[t] + spread\n",
        "                    position = 1\n",
        "\n",
        "            elif action == 2:\n",
        "                if position == 3:\n",
        "                    states = self.trend[t] - spread\n",
        "                    position = 2\n",
        "                elif position == 1:\n",
        "                    pip += ((self.trend[t] - states) * pip_cost)\n",
        "                    states = self.trend[t] - spread\n",
        "                    position = 2\n",
        "\n",
        "            # state = next_state\n",
        "        return pip\n",
        "\n",
        "    def train(self, iterations, checkpoint, spread, pip_cost):\n",
        "        for i in range(iterations):\n",
        "            position = 3\n",
        "            total_pip = 0\n",
        "            max_pip = 0\n",
        "            pip = []\n",
        "            spread = spread / pip_cost\n",
        "            mean_pip = 0.0\n",
        "            done = False\n",
        "            state = self.get_state(0)\n",
        "            for t in  range(len(self.trend) - 28000, len(self.trend)-1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign('actor-original', 'actor-target')\n",
        "                    self._assign('critic-original', 'critic-target')\n",
        "\n",
        "                action = self._select_action(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "\n",
        "                if action == 0:\n",
        "                    if position == 3:\n",
        "                        states = self.trend[t] + spread\n",
        "                        position = 1\n",
        "                    elif position == 2:\n",
        "                        p = (states - self.trend[t]) * pip_cost\n",
        "                        if p <= -40:\n",
        "                            p = -40\n",
        "                        pip.append(p)\n",
        "                        total_pip = sum(pip)\n",
        "                        states = self.trend[t] + spread\n",
        "                        position = 1\n",
        "\n",
        "\n",
        "                elif action == 1:\n",
        "                    if position == 3:\n",
        "                        states = self.trend[t] - spread\n",
        "                        position = 2\n",
        "                    elif position == 1:\n",
        "                        p = (self.trend[t] - states) * pip_cost\n",
        "                        if p <= -40:\n",
        "                            p = -40\n",
        "                        pip.append(p)\n",
        "                        total_pip = sum(pip)\n",
        "                        states = self.trend[t] - spread\n",
        "                        position = 2\n",
        "\n",
        "                total_pip = np.float(total_pip)\n",
        "                if max_pip <= total_pip:\n",
        "                    max_pip = total_pip\n",
        "\n",
        "                if max_pip > total_pip:\n",
        "                    s = max_pip - total_pip\n",
        "                    if s < -200:\n",
        "                      total_pip = -300\n",
        "                      done = True\n",
        "\n",
        "                if total_pip < -300:\n",
        "                    done = True\n",
        "                \n",
        "                if len(pip) != 0:\n",
        "                  mean_pip = np.mean(np.asanyarray(pip))\n",
        "                self._memorize(state, action, mean_pip, next_state, done)\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                state = next_state\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                cost = self._construct_memories_and_train(replay)\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "                if done == True:\n",
        "                  break\n",
        "            trade_accuracy = np.mean(np.asanyarray(pip) > 0)\n",
        "            print('trade accuracy = ', trade_accuracy)\n",
        "            print('epoch: %d, total rewards: %f, cost: %f, mean rewards: %f' % (i + 1, total_pip, cost, mean_pip))\n",
        "\n",
        "    def train1(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in  range(len(self.trend) - 28000, len(self.trend)-1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign('actor-original', 'actor-target')\n",
        "                    self._assign('critic-original', 'critic-target')\n",
        "                \n",
        "                action = self._select_action(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 0 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 1 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                \n",
        "                self._memorize(state, action, invest, next_state, starting_money < initial_money)\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                state = next_state\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                cost = self._construct_memories_and_train(replay)\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhDuMj4_9f34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 50\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(\"drive/My Drive/audpred.csv\", window_size, skip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-90luLXDozf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.train(2000, 10, 10, 1000)\n",
        "# agent.train1(iterations = 200, checkpoint = 10, initial_money = 10000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}